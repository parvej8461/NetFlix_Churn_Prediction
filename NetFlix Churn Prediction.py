# -*- coding: utf-8 -*-
"""notebook7cf3f0c361

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/notebook7cf3f0c361-1991ae95-2d09-438c-9f1c-d393e8cf89ce.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20240815/auto/storage/goog4_request%26X-Goog-Date%3D20240815T065910Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D64c90b13f3d946e01ded06d157c06f0848a91b78165d4efa0ee54bf6cfb44c9759718c539677060ba9fa97a53975f3ee0eb666fd7d7bdecbb55336f3f58f8193fc7849e47d6fbd947cabb391644eb486cc924536219507270a01ca2e4d45642f9c00f812ad66150390f6e4ff0aea4cb7620888dfc25b8d295e8cd174aa538faaed9715f9f21b9bcda6074d712f62ac388d21222cbdcc333c9a9b9132a578a0b554f9304e544295b8802a64f42c95cd372e25928bbab4ad123b4a55cf0dae6dfe1cd53470a37248012edadedf335374bebfdea0665f377bf2f401717971ca946c23532427814ca1367dc9691d9aa69846087f0656adb4cda8eb24a666c8d42bf3

# **What is Churn Prediction?**

Churn prediction is analytical studies on the possibility of a customer abandoning a product or service. The goal is to understand and take steps to change it before the costumer gives up the product or service.

## **About Data**

#### customerID : Customer ID
#### gender : Whether the customer is a male or a female
#### SeniorCitizen : Whether the customer is a senior citizen or not (1, 0)
#### Partner : Whether the customer has a partner or not (Yes, No)
#### Dependents : Whether the customer has dependents or not (Yes, No)
#### tenure : Number of months the customer has stayed with the company
#### PhoneService : Whether the customer has a phone service or not (Yes, No)
#### MultipleLines : Whether the customer has multiple lines or not (Yes, No, No phone service)
#### InternetService : Customer’s internet service provider (DSL, Fiber optic, No)
#### OnlineSecurity : Whether the customer has online security or not (Yes, No, No internet service)
#### OnlineBackup : Whether the customer has online backup or not (Yes, No, No internet service)
#### DeviceProtection : Whether the customer has device protection or not (Yes, No, No internet service)
#### TechSupport : Whether the customer has tech support or not (Yes, No, No internet service)
#### StreamingTV : Whether the customer has streaming TV or not (Yes, No, No internet service)
#### StreamingMovies : Whether the customer has streaming movies or not (Yes, No, No internet service)
#### Contract : The contract term of the customer (Month-to-month, One year, Two year)
#### PaperlessBilling : Whether the customer has paperless billing or not (Yes, No)
#### PaymentMethod : The customer’s payment method (Electronic check, Mailed check, Bank transfer (automatic), Credit card (automatic))
#### MonthlyCharges : The amount charged to the customer monthly
#### TotalCharges : The total amount charged to the customer
#### Churn : Whether the customer churned or not (Yes or No)

![image.png](https://s16353.pcdn.co/wp-content/uploads/2018/06/Churn.png)
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline
import warnings
warnings.simplefilter('ignore')
plt.style.use("fivethirtyeight")

data = pd.read_csv("/content/WA_Fn-UseC_-Telco-Customer-Churn.csv")

data.head()

data.dtypes

data.shape

data.isna().sum()

data.groupby('Churn')[['MonthlyCharges', 'tenure']].agg(['min', 'max', 'mean'])

"""TotalCharges columns has numeric values but looks object type."""

data[data['TotalCharges'] == ' ']

data['TotalCharges'] = data['TotalCharges'].replace(' ', np.nan)

data[data['TotalCharges'] == ' ']

data['TotalCharges'].isna().sum()

data['TotalCharges'] = pd.to_numeric(data['TotalCharges'])

data['TotalCharges'].dtypes

data.groupby('Churn')[['MonthlyCharges', 'tenure', 'TotalCharges']].agg(['min', 'max', 'mean'])

"""Since, we have 11 null values in dataset, either we can fill them, or remove them. 11 is a low number, so I will drop them."""

data.dropna(inplace = True)

data.isna().sum()

data.shape

data.groupby('Churn')[['OnlineBackup', 'OnlineSecurity', 'PhoneService']].count()

def half_corr_heatmap(data, title=None):
    plt.figure(figsize=(9,9))
    sns.set(font_scale=1)

    mask = np.zeros_like(data.corr())
    mask[np.tril_indices_from(mask)] = True

    with sns.axes_style("white"):
        sns.heatmap(data.corr(), mask=mask, annot=True, cmap="coolwarm")

    if title: plt.title(f"\n{title}\n", fontsize=18)
    plt.show()
    return

"""**Churn columns is not there because its dtype is object. Converting objects into a numeric.**"""

data['Churn'] = data['Churn'].map({'Yes' : 1, 'No' : 0})

def corr_for_target(data, target, title=None):
    plt.figure(figsize=(4,14))
    sns.set(font_scale=1)

    sns.heatmap(data.corr()[[target]].sort_values(target, ascending=False)[1:], annot=True, cmap="coolwarm")

    if title: plt.title(f"\n{title}\n", fontsize=18)
    return

sns.countplot(data['InternetService']);

sns.countplot(data['MultipleLines']);

data2 = data.drop(['customerID'], axis = 1)

"""To observe numerical, and numeric columns:"""

numerical = data2.select_dtypes(['number']).columns
print(f'Numerical: {numerical}\n')

categorical = data2.columns.difference(numerical)

data2[categorical] = data2[categorical].astype('object')
print(f'Categorical: {categorical}')

"""Creating ones, and zeros from categorical variables:"""

data2 = pd.get_dummies(data2)

data2.head()

"""Checking unique values of every column:"""

data_cols = data.drop('customerID', axis = 1)

for col in data_cols.columns:
    print(col, "\n")
    print(data[col].unique(), "\n")

plt.figure(figsize = (10,8))

ax = sns.distplot(data['tenure'], rug=True, rug_kws={"color": "g"},
                  kde_kws={"color": "red", "lw": 3},
                  hist_kws={"histtype": "step", "linewidth": 3,
                            "alpha": 0.4, "color": "g"});

"""### There are people staying with this company for about 70 years."""

plt.figure(figsize=(12,8))

sns.distplot(data['MonthlyCharges']);

"""### Most of the customer has low monthly charge."""

data[data['Churn'] == 1].TotalCharges.plot(kind = 'hist', alpha = 0.3, color = '#016a55', label = 'Churn = Yes')

data[data['Churn'] == 0].TotalCharges.plot(kind = 'hist', alpha = 0.3, color = '#d89955', label = 'Churn = No')

plt.xlabel('Total Charges')
plt.legend();

"""### Those with lower total charges have left the brand most."""

data[data['Churn'] == 1].MonthlyCharges.plot(kind = 'hist', alpha = 0.3, color = '#019955', label = 'Churn = Yes')

data[data['Churn'] == 0].MonthlyCharges.plot(kind = 'hist', alpha = 0.3, color = '#d89955', label = 'Churn = No')

plt.xlabel('Monthly Charges')
plt.legend();

data[data['Churn'] == 1].tenure.plot(kind = 'hist', alpha = 0.3, color = '#019955', label = 'Yes')

data[data['Churn'] == 0].tenure.plot(kind = 'hist', alpha = 0.3, color = '#d89955', label = 'No')

plt.xlabel('Tenure')
plt.legend();

"""### Splitting the Data"""

X = data2.drop('Churn', axis=1)

y = data2['Churn']

"""# Model Building"""

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn import metrics
from xgboost import XGBClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import GradientBoostingClassifier
from lightgbm import LGBMClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import StandardScaler

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .33, random_state = 42)

models = []
models.append(('Random Forest Clas.', RandomForestClassifier()))
models.append(('KNN', KNeighborsClassifier()))
models.append(('Decision Tree Clas.', DecisionTreeClassifier()))
models.append(("LightGBM", LGBMClassifier()))
models.append(('GBC',GradientBoostingClassifier()))
models.append(('Logistic Reg.', LogisticRegression()))
models.append(('XGB', XGBClassifier()))
models.append(('SVC', SVC()))

"""Creating a for loop to see cross validation scores for every model above:"""

model_names = []
scores = []

for name, model in models:
    score = cross_val_score(model, X, y, cv = 10, scoring='accuracy')
    scores.append(score)
    model_names.append(name)
    print(f"Mean of the {name} model scores : {score.mean()}")

"""# Feature Importance By LightGBM

Checking the features that are most important for LGBM:
"""

feature_importance = pd.DataFrame({'Importance' : LGBMClassifier().fit(X, y).feature_importances_}, index = X.columns)

feature_importance.sort_values(by = 'Importance', ascending = False, axis = 0)[:5].plot(kind = 'bar', color = '#019955', figsize = (10, 5))
plt.xlabel("Feature Importance by LightGBM", color = "#019955", fontdict= {"fontsize" : 20});

"""Model building with all features:"""

model_lgbm = LGBMClassifier()
model_lgbm.fit(X_train, y_train)

y_pred_lgbm = model_lgbm.predict(X_test)
y_pred_lgbm_train = model_lgbm.predict(X_train)

lgbm_test_as = metrics.accuracy_score(y_pred_lgbm, y_test)
lgbm_train_as = metrics.accuracy_score(y_pred_lgbm_train, y_train)

print(f"LGBM accuracy score for test data {lgbm_test_as}")
print(f"LGBM accuracy score for train data {lgbm_train_as}")

"""#### Accuracy score between train and test data is slightly high.

Let's try again with the new features that we got above.
"""

X_train_new = X_train[['MonthlyCharges', 'TotalCharges', 'tenure', 'PaymentMethod_Electronic check']]

X_test_new = X_test[['MonthlyCharges', 'TotalCharges', 'tenure', 'PaymentMethod_Electronic check']]

new_model_lgbm = LGBMClassifier()
new_model_lgbm.fit(X_train_new, y_train)

new_y_pred = new_model_lgbm.predict(X_test_new)
lgbm_ft_as = metrics.accuracy_score(new_y_pred, y_test)
lgbm_ft_as

new_y_pred_train = new_model_lgbm.predict(X_train_new)
lgbm_ft_as_ = metrics.accuracy_score(new_y_pred_train, y_train)
lgbm_ft_as_

"""Not much thing has changed actually. We couldn't improve our model like we want it to be.

# Logistic Regression
"""

log = LogisticRegression()
log.fit(X_train, y_train)

log_y_pred = log.predict(X_test)
log_y_pred_train = log.predict(X_train)

log_test_as = metrics.accuracy_score(log_y_pred, y_test)
log_train_as = metrics.accuracy_score(log_y_pred_train, y_train)

print(f"Accuracy score for test data : {log_test_as}")
print(f"Accuracy score for train data : {log_train_as}")

print(metrics.classification_report(log_y_pred, y_test))

metrics.confusion_matrix(log_y_pred, y_test)

metrics.confusion_matrix(log_y_pred_train, y_train)

y_proba_log = log.predict_proba(X_test)[:, 1]
fpr, tpr, threshold = metrics.roc_curve(y_test, y_proba_log)

plt.plot([0, 1], [0, 1], 'k--')
plt.plot(fpr, tpr, label = 'Logistic Regression')
plt.xlabel('fpr')
plt.ylabel('tpr')
plt.title('ROC Curve')
plt.legend();

metrics.roc_auc_score(y_test, y_proba_log)

y_proba_log_train = log.predict_proba(X_train)[:, 1]
metrics.roc_auc_score(y_train, y_proba_log_train)

"""# SVC"""

svc = SVC()
svc.fit(X_train, y_train)

y_pred_svc = svc.predict(X_test)
y_pred_train = svc.predict(X_train)

svc_train_as = metrics.accuracy_score(y_train, y_pred_train)
svc_as = metrics.accuracy_score(y_test, y_pred_svc)

print(f"Accuracy score for test data : {svc_as}")
print(f"Accuracy score for train data : {svc_train_as}")

print(metrics.classification_report(y_test, y_pred_svc))

"""Let's try after scaling the data."""

sc = StandardScaler()

X_train_sc = sc.fit_transform(X_train)
X_test_sc = sc.transform(X_test)

svc_sc = SVC()
svc_sc.fit(X_train_sc, y_train)

y_pred_sc = svc_sc.predict(X_test_sc)
y_pred_sc_train = svc_sc.predict(X_train_sc)

svc_sc_train_as = metrics.accuracy_score(y_train, y_pred_sc_train)
svc_sc_as = metrics.accuracy_score(y_test, y_pred_sc)

print(f"Accuracy score for test data : {svc_sc_as}")
print(f"Accuracy score for train data : {svc_sc_train_as}")

params = {'kernel' : ['rbf'], 'C' : [0.1, 1, 5, 10], 'gamma' : [0.01, 0.1, 0.9, 1]}

grid = GridSearchCV(SVC(), params, cv = 5, return_train_score= False)

# grid.fit(X_train_sc, y_train)

# grid.best_params_
# best_params_ : [C = 1, gamma = 0.01, kernel = 'rbf']

# grid.best_score_
# best_score_ : 0.7968569389377085

"""Model tunning with the best params."""

# svc_new = SVC(**grid.best_params_)
svc_new = SVC(C = 1, gamma = 0.01, kernel = 'rbf')
svc_new.fit(X_train_sc, y_train)

y_pred_new = svc_new.predict(X_test_sc)
y_pred_new_train = svc_new.predict(X_train_sc)

svc_new_train_as = metrics.accuracy_score(y_train, y_pred_new_train)
svc_new_as = metrics.accuracy_score(y_test, y_pred_new)

print(f"Accuracy score for test data : {svc_new_as}")
print(f"Accuracy score for train data : {svc_new_train_as}")

import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc, RocCurveDisplay

# Calculate ROC curve
fpr, tpr, thresholds = roc_curve(y_train, svc_new.decision_function(X_train_sc))
roc_auc = auc(fpr, tpr)

# Plot ROC curve
display = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc, estimator_name='SVC')
display.plot()
plt.show()

"""# KNN"""

testscores = []
trainscores = []

for i in range(1, 10):
    model = KNeighborsClassifier(i)
    model.fit(X_train, y_train)

    test_pred = model.predict(X_test)
    train_pred = model.predict(X_train)

    testscores.append(metrics.accuracy_score(y_test, test_pred))
    trainscores.append(metrics.accuracy_score(y_train, train_pred))

plt.plot(range(1, 10), testscores, label = 'Test Scores', color = 'red')

plt.plot(range(1, 10), trainscores, label = 'Train Scores', color = 'blue')

plt.legend();

"""We can choose k as 8."""

knn = KNeighborsClassifier(8)
knn.fit(X_train, y_train)

y_pred_knn = knn.predict(X_test)
y_pred_knn_train = knn.predict(X_train)

knn_as = metrics.accuracy_score(y_test, y_pred_knn)
knn_as_train = metrics.accuracy_score(y_train, y_pred_knn_train)

print(f"Accuracy score for test data : {knn_as}")
print(f"Accuracy score for train data : {knn_as_train}")

metrics.confusion_matrix(y_test, y_pred_knn)

print(metrics.classification_report(y_test, y_pred_knn))

y_proba = knn.predict_proba(X_test)[:, 1]
fpr, tpr, threshold = metrics.roc_curve(y_test, y_proba)

plt.plot([0, 1], [0, 1], 'k--')
plt.plot(fpr, tpr, label = 'KNN')
plt.xlabel('fpr')
plt.ylabel('tpr')
plt.title('ROC Curve')
plt.legend();

metrics.roc_auc_score(y_test, y_proba)

metrics.confusion_matrix(y_pred_knn, y_test)

"""# Decision Tree Classifier"""

decision_tree = DecisionTreeClassifier()
decision_tree.fit(X_train, y_train)

y_pred_dt = decision_tree.predict(X_test)
y_pred_train_dt = decision_tree.predict(X_train)

dt_as = metrics.accuracy_score(y_test, y_pred_dt)
dt_as_train = metrics.accuracy_score(y_train, y_pred_train_dt)

print(f"Accuracy score for test data : {dt_as}")
print(f"Accuracy score for train data : {dt_as_train}")

"""# Random Forest Classifier"""

random_forest = RandomForestClassifier()
random_forest.fit(X_train, y_train)

y_pred_rf = random_forest.predict(X_test)
y_pred_train_rf = random_forest.predict(X_train)

rf_as = metrics.accuracy_score(y_test, y_pred_rf)
rf_as_train = metrics.accuracy_score(y_train, y_pred_train_rf)

print(f"Accuracy score for test data : {rf_as}")
print(f"Accuracy score for train data : {rf_as_train}")

random_forest_ = RandomForestClassifier(100)
random_forest_.fit(X_train, y_train)

y_pred_rf_ = random_forest_.predict(X_test)
y_pred_train_rf_ = random_forest_.predict(X_train)

rf_as_ = metrics.accuracy_score(y_test, y_pred_rf_)
rf_as_train_ = metrics.accuracy_score(y_train, y_pred_train_rf_)

print(f"Accuracy score for test data : {rf_as_}")
print(f"Accuracy score for train data : {rf_as_train_}")

"""Checking feature importance for random forest classifier:"""

feature_importance_ = pd.DataFrame({'Importance' : RandomForestClassifier().fit(X, y).feature_importances_}, index = X.columns)

feature_importance_.sort_values(by = 'Importance', ascending = False, axis = 0)[:5].plot(kind = 'bar', color = '#019955', figsize = (10, 5))
plt.xlabel("Feature Importance by Random Forest Classifier", color = "#019955", fontdict= {"fontsize" : 20});

X_train_new_ = X_train[['MonthlyCharges', 'TotalCharges', 'tenure', 'Contract_Month-to-month', 'OnlineSecurity_No']]
X_test_new_ = X_test[['MonthlyCharges', 'TotalCharges', 'tenure', 'Contract_Month-to-month', 'OnlineSecurity_No']]

random_forest_new = RandomForestClassifier()
random_forest_new.fit(X_train_new_, y_train)

y_pred_rf_new = random_forest_new.predict(X_test_new_)
y_pred_train_rf_new = random_forest_new.predict(X_train_new_)

rf_as_new = metrics.accuracy_score(y_test, y_pred_rf_new)
rf_as_train_new = metrics.accuracy_score(y_train, y_pred_train_rf_new)

print(f"Accuracy score for test data : {rf_as_new}")
print(f"Accuracy score for train data : {rf_as_train_new}")

"""Nothing has changed again.

***Let's try with gridsearchcv to find best parameters.***
"""

params_grid = {'criterion' : ['entropy', 'gini'], 'max_depth' : [2, 4, 6, 8], 'n_estimators' : [300, 400, 500],
              'min_samples_split' : [2, 4, 6, 8], 'min_samples_leaf' : [2, 3, 5, 7]}

gscv_rf = GridSearchCV(RandomForestClassifier(), params_grid, cv = 3, scoring = 'f1')
# gscv_rf.fit(X_train_sc, y_train)

# gscv_rf.best_params_
# {'criterion': 'gini','max_depth': 8,'min_samples_leaf': 2,'min_samples_split': 8,'n_estimators': 400}

# model tunning with best parameters

rf_gscv =RandomForestClassifier(n_estimators = 400, criterion = 'gini', max_depth = 8, min_samples_split = 8, min_samples_leaf = 2)
rf_gscv.fit(X_train_sc, y_train)

y_pred_gsvc = rf_gscv.predict(X_test_sc)
y_pred_gsvc_train = rf_gscv.predict(X_train_sc)

rf_gscv_as = metrics.accuracy_score(y_test, y_pred_gsvc)
rf_gscv_train_as = metrics.accuracy_score(y_train, y_pred_gsvc_train)

print(f"Accuracy score for test data : {rf_gscv_as}")
print(f"Accuracy score for train data : {rf_gscv_train_as}")

metrics.confusion_matrix(y_test, y_pred_gsvc)

metrics.confusion_matrix(y_train, y_pred_gsvc_train)

"""**Now, we do not have an overfitting problem!**

# XGBoost
"""

xg = XGBClassifier()
xg.fit(X_train_sc, y_train)

y_pred_xg = xg.predict(X_test_sc)

y_pred_xg_train = xg.predict(X_train_sc)

xg_as = metrics.accuracy_score(y_test, y_pred_xg)
xg_as_train = metrics.accuracy_score(y_train, y_pred_xg_train)

print(f"Accuracy score of test data : {xg_as}")
print(f"Accuracy score of train data : {xg_as_train}")

import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

# Assuming 'xg', 'X_test_sc', and 'y_test' are already defined

cm = confusion_matrix(y_test, xg.predict(X_test_sc))  # Calculate confusion matrix
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[1, 0])  # Create display object
disp.plot()  # Plot confusion matrix
plt.show()  # Show the plot

!pip install scikit-learn --upgrade # Update scikit-learn to the latest version

import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, RocCurveDisplay

# Assuming 'xg', 'X_test_sc', and 'y_test' are already defined

fpr, tpr, thresholds = roc_curve(y_test, xg.predict_proba(X_test_sc)[:, 1]) # Calculate ROC curve values
roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr).plot() # Create a display object and plot it
plt.show() # Show the plot

parameters = {'learning_rate' : [0.01, 0.03, 0.05], 'max_depth' : [1, 4, 6], 'n_estimators' : [100, 300, 400, 600]}

xg_grid = GridSearchCV(XGBClassifier(), parameters, cv = 5)

# xg_grid.fit(X_train_sc, y_train)

# xg_grid.best_score_
# 0.804287486519285

# xg_grid.best_params_
# {'learning_rate': 0.05, 'max_depth': 1, 'n_estimators': 600}

# Parameters tunning
xg_gridcv =XGBClassifier(learning_rate = .05, max_depth = 1, n_estimators = 600)

xg_gridcv.fit(X_train_sc, y_train)

y_pred_xggrid = xg_gridcv.predict(X_test_sc)
y_pred_xggrid_train = xg_gridcv.predict(X_train_sc)

xg_as_grid = metrics.accuracy_score(y_test, y_pred_xggrid)
xg_as_grid_train = metrics.accuracy_score(y_train, y_pred_xggrid_train)

print(f"Accuracy score of test data : {xg_as_grid}")
print(f"Accuracy score of train data : {xg_as_grid_train}")

"""Now, looks good!"""

from sklearn.metrics import ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# Predict labels for test data
y_pred_xggrid = xg_gridcv.predict(X_test_sc)

# Create the confusion matrix display object
cm_display = ConfusionMatrixDisplay.from_predictions(
    y_test, y_pred_xggrid, cmap='cool', display_labels=[1, 0]
)

# Plot the confusion matrix
cm_display.plot()
plt.show()

import matplotlib.pyplot as plt
from sklearn.metrics import RocCurveDisplay

# Assuming 'xg_gridcv' is your fitted model, 'X_test_sc' is your test data, and 'y_test' are the true labels
RocCurveDisplay.from_estimator(xg_gridcv, X_test_sc, y_test)
plt.show()

"""# Gradient Boosting Classifier"""

grad_boost = GradientBoostingClassifier()

grad_boost.fit(X_train_sc, y_train)

y_pred_grad = grad_boost.predict(X_test_sc)
y_pred_grad_train = grad_boost.predict(X_train_sc)

grad_as = metrics.accuracy_score(y_test, y_pred_grad)
grad_as_train = metrics.accuracy_score(y_train, y_pred_grad_train)

print(f"Accuracy score of test data : {grad_as}")
print(f"Accuracy score of train data : {grad_as_train}")

parameters_grad = {'learning_rate' : [0.01, 0.03, 0.05, 0.1], 'max_depth' : [1, 4, 6], 'n_estimators' : [100, 300, 400, 600, 800]}

grad_grid = GridSearchCV(GradientBoostingClassifier(), parameters_grad, cv = 5, scoring = 'f1')

# grad_grid.fit(X_train_sc, y_train)

# grad_grid.best_params_
# {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 600}

# grad_grid.best_score_
# 0.5984668361905707

# Parameter tunning

grad_grid_ = GradientBoostingClassifier(n_estimators = 600, max_depth = 1, learning_rate = .1)

grad_grid_.fit(X_train_sc, y_train)

y_pred_grad_grid = grad_grid_.predict(X_test_sc)
y_pred_grad_grid_train = grad_grid_.predict(X_train_sc)

grad_grid_as = metrics.accuracy_score(y_test, y_pred_grad_grid)
grad_grid_as_train = metrics.accuracy_score(y_train, y_pred_grad_grid_train)

print(f"Accuracy score of test data : {grad_grid_as}")
print(f"Accuracy score of train data : {grad_grid_as_train}")

from sklearn.metrics import ConfusionMatrixDisplay

ConfusionMatrixDisplay.from_estimator(grad_grid_, X_test_sc, y_test, cmap = 'summer', display_labels = [0, 1]);

from sklearn.metrics import RocCurveDisplay

RocCurveDisplay.from_estimator(grad_grid_, X_test_sc, y_test);
# Use RocCurveDisplay to plot the ROC curve in newer scikit-learn versions

"""## Logistic Regression w/ Scaled Data"""

log_sc = LogisticRegression()
log_sc.fit(X_train_sc, y_train)

y_pred_log_sc = log_sc.predict(X_test_sc)
y_pred_log_sc_ = log_sc.predict(X_train_sc)

log_sc_as = metrics.accuracy_score(y_test, y_pred_log_sc)
log_sc_as_ = metrics.accuracy_score(y_train, y_pred_log_sc_)

print(f"Accuracy score of test data : {log_sc_as}")
print(f"Accuracy score of train data : {log_sc_as_}")

from sklearn.metrics import ConfusionMatrixDisplay

ConfusionMatrixDisplay.from_estimator(log_sc, X_test_sc, y_test, cmap = 'GnBu', display_labels = [0, 1]);
# Use ConfusionMatrixDisplay to plot the confusion matrix in newer scikit-learn versions

""">  * We use KNN, Decision Tree Classifier, Random Forest Classifier, XGBoost Classifier, LGBM, Gradien Boosting Classifier, SVC, and Logistic Regressin."""

print("Logistic Regression results : \n")
print(f"Accuracy score of test data : {log_sc_as}")
print(f"Accuracy score of train data : {log_sc_as_}\n")

print("------------------------------------------------")

print("KNN results : \n")
print(f"Accuracy score for test data : {knn_as}")
print(f"Accuracy score for train data : {knn_as_train}\n")

print("------------------------------------------------")

print("SVC result without parameter tunning : \n")
print(f"Accuracy score for test data : {svc_sc_as}")
print(f"Accuracy score for train data : {svc_sc_train_as}\n")
print("SVC results with parameter tunning : \n")
print(f"Accuracy score for test data : {svc_new_as}")
print(f"Accuracy score for train data : {svc_new_train_as}\n")

print("------------------------------------------------")

print("LGBM results without parameter importance : \n")
print(f"LGBM accuracy score for test data {lgbm_test_as}")
print(f"LGBM accuracy score for train data {lgbm_train_as}\n")
print("LGBM result with feature importance : \n")
print(f"LGBM accuracy score for test data {lgbm_ft_as}")
print(f"LGBM accuracy score for train data {lgbm_ft_as_}\n")

print("------------------------------------------------")

print("Decision Tree Classifier results with parameter importance : \n")
print(f"Accuracy score for test data : {dt_as}")
print(f"Accuracy score for train data : {dt_as_train}\n")

print("------------------------------------------------")

print("Random Forest Classifier without parameter tunning : \n")
print(f"Accuracy score for test data : {rf_as}")
print(f"Accuracy score for train data : {rf_as_train}\n")
print("Random Forest Classifier with parameter tunning : \n")
print(f"Accuracy score for test data : {rf_gscv_as}")
print(f"Accuracy score for train data : {rf_gscv_train_as}\n")

print("------------------------------------------------")

print("XGBoost results without parameter tunning : \n")
print(f"Accuracy score of test data : {xg_as}")
print(f"Accuracy score of train data : {xg_as_train}\n")
print("XGBoost results with parameter tunning : \n")
print(f"Accuracy score of test data : {xg_as_grid}")
print(f"Accuracy score of train data : {xg_as_grid_train}\n")

print("------------------------------------------------")

print("Gradient Boosting Classifier results without parameter tunning : \n")
print(f"Accuracy score of test data : {grad_as}")
print(f"Accuracy score of train data : {grad_as_train}\n")
print("Gradient Boosting Classifier results with parameter tunning : \n")
print(f"Accuracy score of test data : {grad_grid_as}")
print(f"Accuracy score of train data : {grad_grid_as_train}")

